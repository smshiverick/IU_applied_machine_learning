{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning At Scale\n",
    "\n",
    "Data Analytics and Machine Learning at Scale \n",
    "\n",
    "---\n",
    "__Name:__  *Dr. James G. Shanahan*   \n",
    "__Email:__  *James.Shanahan  @ gmail.com   \n",
    "__Quiz:__  Debugging strategies in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Spark cluster \n",
    "Please first choose which Spark cluster backs this notebook to get your SC/sqlContext\n",
    "\n",
    "* Back this notebook by Spark that is running on your local machine in a Container world\n",
    "* [LATER] Back this notebook by Spark that is running an EMR Cluster (note one has to read and write data from/to S3 to run Spark jobs on EMR)\n",
    "* [LATER] Back this notebook by Spark that is rnning on your local machine natively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Spark locally\n",
    "Run the next cell if you wish to launch a Spark cluster on your local machine in a Container world and back this notebook by that cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to run pyspark code in a Jupyter notebook launched from the class environment container, you may encounter errors like this:\n",
    "\n",
    "`NameError: name 'sc' is not defined\n",
    "or\n",
    "\n",
    "ValueError: Cannot run multiple SparkContexts at once;`\n",
    "\n",
    "To avoid both of these and still use examples you find in the pyspark docs, where the Spark Context is always referred to as sc, add this line to your code:\n",
    "\n",
    "`sc = SparkContext.getOrCreate(conf)`\n",
    "\n",
    "Note that conf is a SparkConf instance and will already be available in the container. You can inspect it by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.master', u'local[*]'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.app.name', u'PySparkShell')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell once only to start Spark\n",
    "# Rerunning will cause an error\n",
    "#import pyspark \n",
    "#sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[184, 492, 666, 594, 980]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do something to prove it works\n",
    "import random\n",
    "random.seed(9001)\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)\n",
    "#produces a list of 5 numbers\n",
    "# [129, 477, 193, 759, 914]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count example\n",
    "Create some data by executing the following code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.txt\n",
    "hello hi hi hallo\n",
    "bonjour hola hi ciao\n",
    "nihao konnichiwa ola\n",
    "hola nihao hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello hi hi hallo\r\n",
      "bonjour hola hi ciao\r\n",
      "nihao konnichiwa ola\r\n",
      "hola nihao hello"
     ]
    }
   ],
   "source": [
    "cat wordcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hello', {'hallo': 1, 'konnichiwa': 1, 'ola': 1, 'ciao': 1, 'nihao': 2, 'hi': 3, 'bonjour': 1, 'hello': 2, 'hola': 2})\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "file=open(\"wordcount.txt\",\"r+\")\n",
    "wordcount={}\n",
    "for word in file.read().split():\n",
    "    if word not in wordcount:\n",
    "        wordcount[word] = 1\n",
    "    else:\n",
    "        wordcount[word] += 1\n",
    "print (word,wordcount)\n",
    "file.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES on Inputs to Spark\n",
    "\n",
    "http://spark.apache.org/docs/latest/programming-guide.html\n",
    "All of Sparkâ€™s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use textFile(\"/my/directory\"), textFile(\"/my/directory/*.txt\"), and textFile(\"/my/directory/*.gz\").\n",
    "\n",
    "The textFile method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 64MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUIZ: How many \"konnichiwa\" in this word count example? \n",
    "Run next cell to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'ciao', 1)\n",
      "(u'bonjour', 1)\n",
      "(u'nihao', 2)\n",
      "(u'hola', 2)\n",
      "(u'konnichiwa', 1)\n",
      "(u'hallo', 1)\n",
      "(u'hi', 3)\n",
      "(u'hello', 2)\n",
      "(u'ola', 1)\n"
     ]
    }
   ],
   "source": [
    "# complete word count\n",
    "#Count words in file/directory\n",
    "logFileNAME = 'wordcount.txt'\n",
    "text_file = sc.textFile(logFileNAME)\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "wordCounts = counts.collect()\n",
    "for v in counts.collect():\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize('wordcount.txt')  #distributes the string\n",
    "rdd.first()\n",
    "#rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('wordcount.txt')  #create an RDD\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello hi hi hallo'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging in Spark  \n",
    "\n",
    "* PART 1: Write Mapper/reduce functions as standalone code and debug on a test record (key-value pair)\n",
    "* PART 2: n a multi operation call: break it down and debug step by step on a small test data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PART 1: debug each closure independently with small unit tests\n",
    "Where a closure can be (e.g., mapper/reducer/filter function first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is ia an example of  mapper function (referred to as closure in Spark as this function and \n",
    "# its state will be serialized and shipped to each worker)\n",
    "\n",
    "def mySplitFunction(string):\n",
    "    string.split()\n",
    "mySplitFunction(\"hello hi hi hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "# debug this function to return the first token in a string record\n",
    "# for some reason we get back the first character and not the first string\n",
    "def mySplitFunction(string):\n",
    "    toks = string.split()[0]\n",
    "    return toks[0]\n",
    "\n",
    "#fake out my mapper function and debug\n",
    "print (mySplitFunction(\"hello hi hi hallo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "## debug this function to return the first token in a string record\n",
    "# for some reason we get back the first character and not the first string\n",
    "\n",
    "\n",
    "# solution \n",
    "def mySplitFunction(string):\n",
    "    toks = string.split()[0]\n",
    "    return toks\n",
    "\n",
    "#fake out my mapper function and debug\n",
    "print (mySplitFunction(\"hello hi hi hallo\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2:  In a multi operation call: break it down and debug step by step on a small test data set\n",
    "### Call one operation at a time and take a couple of results (e.g., take(1) and examine \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'hello', u'hi', u'hi', u'hallo', u'bonjour', u'hola', u'hi', u'ciao', u'nihao', u'konnichiwa', u'ola', u'hola', u'nihao', u'hello']\n"
     ]
    }
   ],
   "source": [
    "# output the tokens from each record (one to MANY transformation)\n",
    "\n",
    "def mySplitFunction(string):\n",
    "    string.split()\n",
    "    \n",
    "logFileNAME = 'wordcount.txt'\n",
    "text_file = sc.textFile(logFileNAME)\n",
    "\n",
    "#debug flatmap\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).take(20)\n",
    "print counts\n",
    "\n",
    "#              .map(lambda word: (word, 1)) \\\n",
    "#              .reduceByKey(lambda a, b: a + b)\n",
    "# wordCounts = counts.collect()\n",
    "# for v in counts.collect():\n",
    "#     print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'hello', 1), (u'hi', 1), (u'hi', 1)]\n"
     ]
    }
   ],
   "source": [
    "# output the tokens and corresponding count from each record (one to one map function)\n",
    "\n",
    "def mySplitFunction(string):\n",
    "    string.split()\n",
    "    \n",
    "logFileNAME = 'wordcount.txt'\n",
    "text_file = sc.textFile(logFileNAME)\n",
    "\n",
    "#debug flatmap\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .take(3)\n",
    "print counts\n",
    "\n",
    "#              .reduceByKey(lambda a, b: a + b)\n",
    "# wordCounts = counts.collect()\n",
    "# for v in counts.collect():\n",
    "#     print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'ciao', 1)\n",
      "(u'bonjour', 1)\n",
      "(u'nihao', 2)\n",
      "(u'hola', 2)\n",
      "(u'konnichiwa', 1)\n",
      "(u'hallo', 1)\n",
      "(u'hi', 3)\n",
      "(u'hello', 2)\n",
      "(u'ola', 1)\n"
     ]
    }
   ],
   "source": [
    "# complete word count\n",
    "#Count words in file/directory\n",
    "logFileNAME = 'wordcount.txt'\n",
    "text_file = sc.textFile(logFileNAME)\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "wordCounts = counts.collect()\n",
    "for v in counts.collect():\n",
    "    print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'ciao', 1), (u'bonjour', 1), (u'nihao', 2), (u'hola', 2), (u'konnichiwa', 1), (u'hallo', 1), (u'hi', 3), (u'hello', 2), (u'ola', 1)]\n"
     ]
    }
   ],
   "source": [
    "print wordCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__sortByKey([ascending], [numTasks])__\t\n",
    "\n",
    "When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ciao', 1),\n",
       " (u'bonjour', 1),\n",
       " (u'nihao', 2),\n",
       " (u'hola', 2),\n",
       " (u'konnichiwa', 1),\n",
       " (u'hallo', 1),\n",
       " (u'hi', 3),\n",
       " (u'hello', 2),\n",
       " (u'ola', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ola', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Last 1\n",
    "wordCounts[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ciao', 1),\n",
       " (u'bonjour', 1),\n",
       " (u'nihao', 2),\n",
       " (u'hola', 2),\n",
       " (u'konnichiwa', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first  5\n",
    "wordCounts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "285px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "758px",
    "left": "0px",
    "right": "1566.2px",
    "top": "109px",
    "width": "248px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
